{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install skforecast\nimport pandas as pd\nfrom datetime import datetime\nimport math\nfrom sklearn.metrics.pairwise import manhattan_distances # manhattan_distances(X, Y=None, *, sum_over_features=True)[source]\nfrom datetime import datetime\nimport math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport shutil\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n\ndef mkdir(d):\n    if not os.path.exists(d):\n        os.makedirs(d)\n        \nfrench_holidays = [\n    \"2020-07-14\",\n    \"2020-08-15\",\n    \"2020-09-22\",\n    \"2020-11-01\",    \n    \"2020-11-11\",\n    \"2020-12-21\",  \n    \"2020-12-24\", \n    \"2020-12-25\", \n    \"2020-12-26\", \n    \"2020-12-31\",     \n    \n    \"2021-01-01\",\n    \"2021-03-20\",\n    \"2021-04-02\",\n    \"2021-04-04\",\n    \"2021-04-05\",\n    \"2021-05-01\",\n    \"2021-05-08\",\n    \"2021-05-13\",\n    \"2021-05-23\",\n    \"2021-05-24\",\n    \"2021-05-30\",\n    \"2021-06-20\",\n    \"2021-06-21\",\n    \"2021-07-14\",\n    \"2021-08-15\",    \n    \"2021-09-22\",    \n    \"2021-11-01\",    \n    \"2021-11-11\",     \n    \"2021-12-21\",  \n    \"2021-12-24\", \n    \"2021-12-25\", \n    \"2021-12-26\", \n    \"2021-12-31\", \n]","metadata":{"execution":{"iopub.status.busy":"2022-12-14T19:44:06.650286Z","iopub.execute_input":"2022-12-14T19:44:06.650712Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: skforecast in /opt/conda/lib/python3.7/site-packages (0.6.0)\nRequirement already satisfied: joblib<1.3.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from skforecast) (1.2.0)\nRequirement already satisfied: numpy<1.24,>=1.20 in /opt/conda/lib/python3.7/site-packages (from skforecast) (1.21.6)\nRequirement already satisfied: optuna<3.1,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from skforecast) (3.0.3)\nRequirement already satisfied: pandas<1.6,>=1.2 in /opt/conda/lib/python3.7/site-packages (from skforecast) (1.3.5)\nRequirement already satisfied: scikit-optimize==0.9.0 in /opt/conda/lib/python3.7/site-packages (from skforecast) (0.9.0)\nRequirement already satisfied: tqdm<4.65,>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from skforecast) (4.64.0)\nRequirement already satisfied: scikit-learn<1.2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from skforecast) (1.0.2)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize==0.9.0->skforecast) (1.7.3)\nRequirement already satisfied: pyaml>=16.9 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize==0.9.0->skforecast) (21.10.1)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (1.4.39)\nRequirement already satisfied: importlib-metadata<5.0.0 in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (4.13.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (6.7.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (6.0)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (1.8.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (21.3)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (0.8.2)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna<3.1,>=2.10.0->skforecast) (3.10.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas<1.6,>=1.2->skforecast) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas<1.6,>=1.2->skforecast) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<1.2,>=1.0->skforecast) (3.1.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna<3.1,>=2.10.0->skforecast) (1.2.3)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna<3.1,>=2.10.0->skforecast) (5.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna<3.1,>=2.10.0->skforecast) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna<3.1,>=2.10.0->skforecast) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna<3.1,>=2.10.0->skforecast) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas<1.6,>=1.2->skforecast) (1.15.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna<3.1,>=2.10.0->skforecast) (1.1.2)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<3.1,>=2.10.0->skforecast) (3.3.0)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<3.1,>=2.10.0->skforecast) (3.5.1)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<3.1,>=2.10.0->skforecast) (0.5.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<3.1,>=2.10.0->skforecast) (5.10.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<3.1,>=2.10.0->skforecast) (2.4.2)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna<3.1,>=2.10.0->skforecast) (1.8.2)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna<3.1,>=2.10.0->skforecast) (21.4.0)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna<3.1,>=2.10.0->skforecast) (0.2.5)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna<3.1,>=2.10.0->skforecast) (2.1.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reading input files\ntrain_path = '../input/estationdata/public_data/train.csv'\ntest_path = '../input/estationdata/public_data/test.csv'\n\ntrain = pd.read_csv(train_path, sep=\",\")\ntrain['date'] = pd.to_datetime(train['date'])\ntrain = train.drop(['Postcode'], axis=1)\n\n# Removing data before corona restrictions\ncorona_date = datetime.strptime(\"2020-10-18 00:00:00\", '%Y-%m-%d %H:%M:%S')  \ntrain = train[train['date'] > corona_date]\n\ntest = pd.read_csv(test_path, sep=\",\")\ntest['date'] = pd.to_datetime(test['date'])\ntest = test.drop(['Postcode'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stations = train['Station'].unique()\ndfs = []\n\nfor station in stations[:]: # Let's add all the missing datapoints back into the dataframe and fill the known nan's (station, area...)\n    single_station_df = train[train['Station'] == station]\n    single_station_df = single_station_df.set_index('date').asfreq('15min')\n    where_nan = single_station_df.isnull().any(axis=1)\n    \n    area = single_station_df['area'].value_counts().index.tolist()[0]\n    latitude = single_station_df['Latitude'].value_counts().index.tolist()[0]\n    longitude = single_station_df['Longitude'].value_counts().index.tolist()[0]\n\n    single_station_df['Station'] = single_station_df['Station'].fillna(station)\n    single_station_df['area'] = single_station_df['area'].fillna(area)\n    single_station_df['Latitude'] = single_station_df['Latitude'].fillna(latitude)\n    single_station_df['Longitude'] = single_station_df['Longitude'].fillna(longitude)\n\n    single_station_df[\"imputed\"] = np.where(where_nan, 1, 0)\n    single_station_df['date'] = single_station_df.index\n    dfs.append(single_station_df)\n    \ntrain = pd.concat(dfs, ignore_index=True).sort_values(by=['date'])\n\nfor df in [train, test]:\n    df['tod'] = (df.date.dt.hour * 60 + df.date.dt.minute) / 15\n    df['dow'] = df.date.dt.dayofweek\n    df['month'] = df.date.dt.month\n    df['dayofyear'] = df.date.dt.dayofyear\n    df['year'] = df.date.dt.year\n    \n    df['year'] = df['year'].replace([2020], 0)\n    df['year'] = df['year'].replace([2021], 1)\n    df['year'] = df['year'].replace([2022], 2)\n    \n    df['tod_sin'] = np.sin(2 * np.pi * df['tod']/96.0)\n    df['tod_cos'] = np.cos(2 * np.pi * df['tod']/96.0)\n\n    df['dow_sin'] = np.sin(2 * np.pi * df['dow']/7.0)\n    df['dow_cos'] = np.cos(2 * np.pi * df['dow']/7.0)\n\n    df['month_sin'] = np.sin(2 * np.pi * df['month']/12.0)\n    df['month_cos'] = np.cos(2 * np.pi * df['month']/12.0)\n\n    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear']/365.0)\n    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear']/365.0)\n    \n    df['holiday'] = df['date'].isin(french_holidays).astype(int)\n\n    \ntest['imputed'] = 0\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [\"Available\",\"Charging\",\"Passive\",\"Other\"] \n\nexog_variables_local = ['tod_sin', 'tod_cos',\n                  'dow_sin', 'dow_cos', \n                  'month_sin', 'month_cos', \n                  'dayofyear_sin', 'dayofyear_cos', \n                  'year', 'imputed', 'holiday', \n                  'trend']\n\nexog_variables_global = ['Latitude', 'Longitude']\n\nval_local = False # we can set this flag to true to split our dataset into a train and validation set for local evaluation\nif val_local: # train val split \n    split_date = datetime.strptime(\"2021-02-14 00:00:00\", '%Y-%m-%d %H:%M:%S')  \n    test = train[train['date'] > split_date]    \n    train = train[train['date'] <= split_date]\n    test[targets[0] + '_GT'] = test[targets[0]]\n    test[targets[1] + '_GT'] = test[targets[1]]\n    test[targets[2] + '_GT'] = test[targets[2]]\n    test[targets[3] + '_GT'] = test[targets[3]]\n    test = test.drop(targets, axis=1)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Impute","metadata":{}},{"cell_type":"code","source":"# impute train\nwhere_not_null = train.imputed == 0 # add an indicator for values that we impute\nwhere_null = train.imputed == 1\n\ntrain = train.dropna() # Let's drop na, it works better than any imputation method\n\n'''\n# Methods that did not work for imputation\n\n#train[targets] = train[targets].fillna(train.groupby(['date', 'area'])[targets].transform('mean')) # Fill by mean\n#train[targets] = train[targets].fillna(train.groupby(['date'])[targets].transform('median')) # Fill by mean\n\n#for station in stations[:]:\n#    at_train_station = train['Station'] == station\n#    train[at_train_station] = train[at_train_station].fillna(method='bfill')\n    #train[at_train_station] = train[at_train_station].interpolate(method='spline', order=5)\n    \n#train = train.fillna(method='ffill')\n#train = train.interpolate(method='nearest')\n#train = train.interpolate(method='spline', order=5)\n#train = train.interpolate(method='time')\n'''\n\ntrain.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train & Predict","metadata":{}},{"cell_type":"markdown","source":"#### XGBRegressor","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom statsmodels.tsa.arima.model import ARIMA\n\ndef train_predict_for_station(train_df_single_station, predict_df_single_station, num, val_local):\n    prediction_interval_length = len(predict_df_single_station)\n    regressor = xgb.XGBRegressor(n_estimators=1)\n    forecaster  = ForecasterAutoreg(regressor=regressor, lags=20)\n\n    predictions_xgb_regressor = []\n    predictions_arima = []\n    \n    for target in ['Available', \"Charging\",\"Passive\", \"Other\"]:\n        #predict with xgboost\n        forecaster.fit(train_df_single_station[target].rolling(10).mean().iloc[10:], \n                       exog=train_df_single_station[exog_variables_local].iloc[10:])\n        target_predictions_xgb = forecaster.predict(prediction_interval_length, exog=predict_df_single_station[exog_variables_local])\n        predictions_xgb_regressor.append(target_predictions_xgb)\n        \n        # predict with arima\n        model = ARIMA(train_df_single_station[target].rolling(10).mean().iloc[10:].reset_index(drop=True), order=(2, 1, 1))\n        fitted_arima = model.fit()\n        target_predictions_argima = fitted_arima.forecast(prediction_interval_length)\n        predictions_arima.append(target_predictions_argima)\n        \n    return predictions_xgb_regressor, predictions_arima","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, station in enumerate(stations[:]):\n    at_train_station = train['Station'] == station\n    at_test_station = test['Station'] == station\n    \n    predictions_xgb_regressor, predictions_arima = train_predict_for_station(train[at_train_station], test[at_test_station], i, val_local)\n    \n    # put preds into dataframe\n    index = test[at_test_station].index\n    test.loc[at_test_station, 'Available_xgb_regressor'] = pd.Series(predictions_xgb_regressor[0].values, index=index)\n    test.loc[at_test_station, 'Charging_xgb_regressor'] = pd.Series(predictions_xgb_regressor[1].values, index=index)\n    test.loc[at_test_station, 'Passive_xgb_regressor'] = pd.Series(predictions_xgb_regressor[2].values, index=index)\n    test.loc[at_test_station, 'Other_xgb_regressor'] = pd.Series(predictions_xgb_regressor[3].values, index=index)\n    \n    test.loc[at_test_station, 'Available_arima'] = pd.Series(predictions_arima[0].values, index=index)\n    test.loc[at_test_station, 'Charging_arima'] = pd.Series(predictions_arima[1].values, index=index)\n    test.loc[at_test_station, 'Passive_arima'] = pd.Series(predictions_arima[2].values, index=index)\n    test.loc[at_test_station, 'Other_arima'] = pd.Series(predictions_arima[3].values, index=index)\n        \n    print(\"Done training station\", station)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGB Classifier","metadata":{}},{"cell_type":"code","source":"# Create Classes from targets\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nconcat = train['Available'].astype(int).astype(str) + train['Charging'].astype(int).astype(str) + train['Passive'].astype(int).astype(str) + train['Other'].astype(int).astype(str)\nunique_combos = [c for c in concat.unique() if np.sum([int(digit) for digit in list(c)]) == 3]\n\nassert len(unique_combos) == 20\n\nunique_combos.sort()\nunique_combos.reverse()\nclass_dict = {c: idx for idx, c in enumerate(unique_combos)}\nclass_dict\n\ndef targets_to_class(target_values):\n    key = ''.join([str(t) for t in target_values])\n    return class_dict[key]\n    \nfor df in [train]:\n    df['class'] = df.progress_apply(lambda row: targets_to_class([int(row[target]) for target in targets]), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot-encode stations\nstation_columns = [f'Station_{station}' for station in stations]\n\ntrain_station = train['Station']\ntest_station = test['Station']\n\ntrain = pd.get_dummies(train, columns=['Station'])\ntrain['Station'] = train_station\n\ntest = pd.get_dummies(test, columns=['Station'])\ntest['Station'] = test_station\n\n# one hot encode are\narea_columns = [f'area_{area}' for area in train['area'].unique()]\n\ntrain_area = train['area']\ntest_area= test['area']\n\ntrain = pd.get_dummies(train, columns=['area'])\ntrain['area'] = train_area\n\ntest = pd.get_dummies(test, columns=['area'])\ntest['area'] = test_area\n\nexog_variables = exog_variables_local + exog_variables_global + station_columns + area_columns\n\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = xgb.XGBClassifier(n_estimators=1) \nclassifier.fit(train[exog_variables], train['class'], eval_set=[(train[exog_variables], train['class'])])\npredictions = classifier.predict(test[exog_variables])\n\ntest['predicted_class'] = predictions\n\ndef class_to_target(clazz: int, target: str) -> int:\n    target_str = list(class_dict.keys())[list(class_dict.values()).index(clazz)]\n    return int(target_str[targets.index(target)])\n\nfor t in targets:\n    test[t + \"_xgb_classifier\"] = test['predicted_class'].apply(lambda pred: class_to_target(pred, target=t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensembling the predictions and scaling them","metadata":{}},{"cell_type":"code","source":"xgbr_preds = ['Available_xgb_regressor', 'Charging_xgb_regressor', 'Passive_xgb_regressor', 'Other_xgb_regressor']\narima_preds = ['Available_arima', 'Charging_arima', 'Passive_arima', 'Other_arima']\nxgbc_preds = ['Available_xgb_classifier', 'Charging_xgb_classifier', 'Passive_xgb_classifier', 'Other_xgb_classifier']\n\ndef scale_nums_to_sum(nums, station, value=3):\n    if np.sum(nums) == 0:\n        return [0, 0, 0, 0]\n    return [num / np.sum(nums) * value for num in nums]\n\n\ndef round_and_rescale(df, cols):\n    for col in cols: \n        df.loc[test[col] < 0, col] = 0\n        df.loc[test[col] > 3, col] = 3\n    df[cols] = df[cols].apply(np.round)\n    \n    k = df.apply(lambda row : scale_nums_to_sum([row[cols[0]],\n                                                 row[cols[1]], \n                                                 row[cols[2]],\n                                                 row[cols[3]]],\n                                                 row[\"Station\"]), axis = 1)\n    k = np.array(k.to_list())\n\n    df[cols[0]] = k[:, 0]\n    df[cols[1]] = k[:, 1]\n    df[cols[2]] = k[:, 2]\n    df[cols[3]] = k[:, 3]\n    \n    return df\n\n\ndef rescale_and_round(df, cols):\n    for col in cols: \n        df.loc[test[col] < 0, col] = 0\n        df.loc[test[col] > 3, col] = 3\n    \n    \n    k = df.apply(lambda row : scale_nums_to_sum([row[cols[0]],\n                                                 row[cols[1]], \n                                                 row[cols[2]],\n                                                 row[cols[3]]],\n                                                 row[\"Station\"]), axis = 1)\n    k = np.array(k.to_list())\n\n    df[cols[0]] = k[:, 0]\n    df[cols[1]] = k[:, 1]\n    df[cols[2]] = k[:, 2]\n    df[cols[3]] = k[:, 3]\n    \n    df[cols] = df[cols].apply(np.round)\n    \n    return df\n\ntest = rescale_and_round(test, xgbr_preds)\ntest = round_and_rescale(test, arima_preds)\n\nfor xgbr_pred, arima_pred, xgbc_pred, target in zip(xgbr_preds, arima_preds, xgbc_preds, targets):    \n    test[target] = test[arima_pred] * 0.4 + test[xgbr_pred] * 0.35 + test[xgbc_pred] * 0.25\n    \ntest = rescale_and_round(test, targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nimport os\n\nif val_local: # Let's check how we did on the validation set\n    station_maes = []\n    for station in stations:\n        at_station = test['Station'] == station\n        target_maes = []\n\n        for target in targets:\n            error = mean_absolute_error(test.dropna()[at_station][target], test.dropna()[at_station][target + '_GT'])\n            target_maes.append(error)\n\n        station_maes.append(np.sum(target_maes))\n\n    print(\"All stations mea:\", np.mean(station_maes))\n    plt.figure(figsize=(15,20))\n    plt.barh(stations, station_maes)\n    plt.show()\n    \nelse: # Let's make a submission!\n    pred_area = test.groupby(['date', 'area']).agg({\n        'Available': 'sum',\n        'Charging': 'sum',\n        'Passive': 'sum',\n        'Other': 'sum'}).reset_index()\n\n    pred_global = test.groupby('date').agg({\n        'Available': 'sum',\n        'Charging': 'sum',\n        'Passive': 'sum',\n        'Other': 'sum'}).reset_index()\n    \n    mkdir(\"result\")\n\n    test[[\"date\",\"area\",\"Station\",\"Available\",\"Charging\",\"Passive\",\"Other\"]].reset_index(drop=True).to_csv(\"./result/station.csv\", index=False)\n    test[[\"date\",\"area\",\"Available\",\"Charging\",\"Passive\",\"Other\"]].reset_index(drop=True).to_csv(\"./result/area.csv\", index=False)\n    test[[\"date\",\"Available\",\"Charging\",\"Passive\",\"Other\"]].reset_index(drop=True).to_csv(\"./result/global.csv\", index=False)\n    shutil.make_archive(\"./result\",'zip', \"./result\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}